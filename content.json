{"pages":[],"posts":[{"title":"深度学习中防止过拟合的几种方法","text":"三种防止过拟合的办法：Early stopping, Regularization, Dropout Early stopping 在Validation set上的Loss最小时中止 Regularization L1 norm和L2 norm均可用于Regularization中，其中L2 norm较为常用，其如下式， 对比L1和L2的update过程： 其中Sgn(阶跃函数)为绝对值的导数。 L1和L2，虽然它们同样是让参数的绝对值变小，但它们做的事情其实略有不同： L1使参数绝对值变小的方式是每次update减掉一个固定的值 L2使参数绝对值变小的方式是每次update乘上一个小于1的固定值 因此，当参数的绝对值比较大的时候，L2会让下降得更快，而L1每次update只让w减去一个固定的值，train完以后可能还会有很多比较大的参数；当参数的绝对值比较小的时候，L2的下降速度就会变得很慢，train出来的参数平均都是比较小的，而L1每次下降一个固定的value，train出来的参数是比较sparse的，这些参数有很多是接近0的值，也会有很大的值。 Q：为什么要让参数w绝对值变小？ A：正则项有选择地让某些 变小，样本中的特征有很多，但大部分特征都是无关紧要的，只有一小部分关键的特征支撑起了整个预测模型。正则化让无关紧要的变小，而作为关键特征的如果变小，则可能导致损失函数急剧增大，么由此造成的损失函数的扩大将远大于从正则项上获得的微小收益，所以这些关键的可以几乎不受正则项的干涉。 Dropout Dropout的思想和集成学习十分类似，在training的时候，每次update参数之前，我们对每一个neuron(除了最后的output layer)做sampling(抽样) ，每个neuron都有p%的几率会被丢掉，如果某个neuron被丢掉的话，跟它相连的weight也都要被丢掉，实际上就是每次update参数之前都通过抽样只保留network中的一部分neuron来做训练，最后将多次训练的不同参数乘以系数(1-p%)构成完整的网络。 如果network是非linear的，ensemble和dropout是不equivalent的；但是，dropout最后一个很神奇的地方是，虽然在non-linear的情况下，它是跟ensemble不相等的，但最后的结果还是会work如果network很接近linear的话，dropout所得到的performance会比较好，而ReLU和Maxout的network相对来说是比较接近于linear的，所以我们通常会把含有ReLU或Maxout的network与Dropout配合起来使用。","link":"/2021/01/23/2020.1.23(2)/"},{"title":"The foudation of Recurrent Neural Network","text":"Recurrent Neural Network是一种具有记忆能力的神经网络，主要用途在NLP上，句子中的单词意思依赖于前后句子，所以需要使用RNN这种带有记忆能力的神经网络模型。 Bidirectional RNN是双向的RNN，普通的RNN句子中单词的意义只依赖于之前的单词，而如果使用Bidirectional RNN神经网络则具有看整个句子的能力。 Long short-term Memory(LSTM) 目前来说，一般提到的RNN都指LSTM，其结构如下， LSTM通过三个gate来控制输入输出以及是否遗忘，可以与RNN的模型比较着看。 Training RNN RNN的训练过程中Loss会剧烈抖动，画出它的图像可以看出会存在“悬崖”与“平原”，如图所示， 这是一个简单的例子解释为什么RNN会有这样的特性 故RNN经常出现梯度消失和梯度爆炸。 LSTM可以拿掉error surface上比较平坦的地方，从而解决梯度消失的问题。 Q：为什么要把RNN换成LSTM？ A：LSTM可以解决梯度消失的问题 Q：为什么LSTM能够解决梯度消失的问题？ A：RNN和LSTM对memory的处理其实是不一样的： 在RNN中，每个新的时间点，memory里的旧值都会被新值所覆盖 在LSTM中，每个新的时间点，memory里的值会乘上\\(f(g_f)\\)与新值相加 对RNN来说，w对memory的影响每次都会被清除，而对LSTM来说，除非forget gate被打开，否则\\(w\\)对memory的影响就不会被清除，而是一直累加保留，因此它不会有梯度消失的问题。","link":"/2021/01/25/2020.1.25/"},{"title":"The foundation of Convolutional Neural network","text":"CNN用在图像处理或类似方式表达的问题当中(比如棋盘)，一般性流程如下， Convolution Convolution的本质是在一个图像中不断移动一个小框，然后看这个小框内是否存在一些特征。 自动学习出多个Filter，然后每个Filter原来图像的matrix中移动，计算内积得到新的matrix，最后得到一个多维的结果，过程如下， 注：如果原图像有多个维度，则每个Filter也有同样的维度。 Filter的过程通过神经网络实现， MAX Pooling MAX Pooling的过程类似于抽调图片的一些像素，图像整体变化不大，人类仍然可以识别，但是这个操作却减小了计算量。 MAX Pooling的过程是压缩Convolution得到的结果，用最大的值代替一个区域的结果。整个CNN的过程如下， What does CNN learn？ 深度学习并非完全不可理解，但是它学习到的东西和人理解的确实有所差异，比如手写数字识别问题，用训练好的网络反推最佳匹配的输入应该是什么，发现和想象的相差巨大， 运用之妙，存乎一心 比如围棋，围棋如果进行MAX Pooling操作则会丢失掉许多棋子的信息，那么我们在CNN训练下围棋时就应该删去MAX Pooling操作，而事实上Alpha Go也是这么做的。 再比如这样的语音处理，Time上会有后续操作进行处理，而在Frequency上男女生发音音调不同，但是不影响内容一致，故在Frequency上使用Filter寻找特征。 词向量模型使用CNN也是同理，不需要像红色框一样进行纵向的查找。 针对不同的application要设计符合它特性的network structure，而不是生硬套用，这就是CNN架构的设计理念： 运用之妙，存乎一心","link":"/2021/01/23/2021.1.23(1)/"},{"title":"Semi-supervised","text":"在带标签的数据之外，有一组无标签数据用于模型学习。通常无标签数据远多于有标签数据。 将无标签数据作为测试数据不是作弊，因为没有使用无标签数据的标签，而使用的是它的特征。 为什么使用半监督学习？ 收集数据很容易，但收集有标签数据是有较高代价的。 -人类的学习是半监督的。 这里介绍半监督学习其中的两个假设 Low-density Separation Assumption 假设世界非黑即白，那么我们的分界应该很清晰。 如下图，左边的分界就比右边的好。 Smoothness Assumption 近朱者赤近墨者黑 如下图，尽管可能里更近，但是有连续的点过渡到，所以更可能和是一类。","link":"/2021/02/01/2021.2.1/"},{"title":"DiagnoseRE复现","text":"本周对上周的代码进行了部分复现，虽然Github已经提供了源代码，但是相比于之前直接跑别人完整的程序代码还是有有一定难度的，在此记录复现的过程。 论文地址：https://arxiv.org/abs/2009.06206 项目地址：https://github.com/zjunlp/DiagnoseRE 由于本地无NVIDIA独显，故采用Colab进行。 准备工作 git下载源代码 git clone https://github.com/zjunlp/DiagnoseRE.git 进入目录安装依赖 12cd DiagnoseREpip install -r requirements.txt 安装OpenNRE（一个开源的关系提取框架，项目地址https://github.com/thunlp/OpenNRE） 123git clone https://github.com/thunlp/OpenNRE.gitpip install -r requirements.txtpython setup.py install 利用OpenNRE自带的脚本下载wiki80数据集 bash benchmark/download_wiki80.sh 注：由于TACRED数据集下载需注册付费，故只采用wiki80进行实验。 开始训练 train.py的使用方法已在文档中给出， 123456789101112131415161718192021222324252627python train.py -husage: train.py [-h] --model_path MODEL_PATH [--restore] --train_path TRAIN_PATH --valid_path VALID_PATH --relation_path RELATION_PATH [--num_epochs NUM_EPOCHS] [--max_seq_len MAX_SEQ_LEN] [--batch_size BATCH_SIZE] [--metric {micro_f1,acc}]optional arguments: -h, --help show this help message and exit --model_path MODEL_PATH, -m MODEL_PATH Full path for saving weights during training --restore Whether to restore model weights from given model path --train_path TRAIN_PATH, -t TRAIN_PATH Full path to file containing training data --valid_path VALID_PATH, -v VALID_PATH Full path to file containing validation data --relation_path RELATION_PATH, -r RELATION_PATH Full path to json file containing relation to index dict --num_epochs NUM_EPOCHS, -e NUM_EPOCHS Number of training epochs --max_seq_len MAX_SEQ_LEN, -l MAX_SEQ_LEN Maximum sequence length of bert model --batch_size BATCH_SIZE, -b BATCH_SIZE Batch size for training and testing --metric {micro_f1,acc} Metric chosen for evaluation 进入命令行，填入参数即可开始训练，relation是一个关系字符串映射为数字的json文件，通过查看源代码，num_epochs的默认值为10，batch_size为64。 这里saved_model是自己创建的文件名用于保存模型的训练参数，其他path填wiki80数据集对应的文件即可。 1python train.py -m saved_model -v OpenNRE/wiki80/wiki80_val.txt -t OpenNRE/wiki80/wiki80_train.txt -r OpenNRE/wiki80/wiki80_rel2id.json 训练用时较长，由于是第一次进行类似的工作，经验不足，比如不知道model_path是填文件地址还是目录地址，而这种情况需要训练完第一个epoch后才会报错。以后进行代码调试的时候可以减少batch size或者只取一小部分训练样本用于测试，这样加快训练速度，代码有问题可以很快发现。 实验结果 训练了近3个小时，得到的f1 score为86.2857%，与论文中的86.2%基本一致。","link":"/2021/03/07/DiagnoseRE%E5%A4%8D%E7%8E%B0/"},{"title":"关于博客","text":"2021.5.15之前的内容是以前写的，很多是为了自己记忆的所以内容比较跳跃。 搭建这个博客是希望督促自己学习，将学习内容归档，也可算是学习过程的一个见证。","link":"/2021/05/15/2021.5.15/"},{"title":"NLP&amp;KG前沿论坛讲座笔记","text":"Part 1 AI的三种方法 分别为符号主义、统计方法、连接主义 符号主义：易于解释，难以学习 统计方法：可解释性尚可，表达能力相对较弱，可以学习的 连接主义（神经网络）：效果好，难以解释 三种方法的结合 正则表达式与神经网络相结合 内容 作者介绍的是将正则表达式与神经网络相结合。 单一正则表达式系统的优点： 缺点： 将正则表达式变成神经网络，就可以将其优点融入神经网络中，那么得到的模型就拥有了如下优点， forward的方法，其本身过程很像RNN， 于是，作者对于FA做了一些修改，得到FA-RNN。（具体过程可以参考论文） 将传统RE system的Logic进行soft操作， 实验结果： 模型可以还原出正则表达式，PPT中左图为原始的正则表达式，右图为还原出的，虽然有一些差别，但是基本符合。 小结 统计推理融入神经网络 内容 涉及到的一些概念：dependency parsing（依存句法分析） 缺少相应基础，中间的推导没能听懂，讲的是MF，然后这个过程比较像GNN，所以作者就将GNN与之结合起来构建模型。 小结 小结 Part 2 知识图谱：一种结点和边的有向图结构。 知识图谱相对于语义网弱化了本体的概念， 知识融合：多模态、跨语言、不一致性（真值推断） （模态：每一种信息的来源或者形式，都可以称为一种模态） 本体匹配与实体对齐在很多研究中相互促进。 把人引入实体对齐，先用机器学习生成一个初始的结果，然后通过人来检验不停的循环提高模型结果。 困难：知识图谱的数据不够直观，要选择合适的人，在有限的循环次数中效果最好，结果传播…… 传递方法或偏序进行对齐推断，这样就可以不用人来推断 作者的工作解决的主要是结果的传播 将多个图合并起来，将可能实体对齐的对放在候选集里，然后计算概率，把最有可能实体对齐对交给人去判断，形成闭环。 知识融合 还需要建模数据源的质量。 长尾实体：三元组数量少的实体占了所有实体的大部分，这些实体被叫做长尾实体。 介绍比较火的基于表示学习的方法 由于异构性不完备等原因，所以学习表示就遇到了困难。比如有的图谱没有爷爷的关系，表示为父亲的父亲，那么就是二阶邻居，但在有爷爷关系的图谱中就是一阶邻居。 Part 3 人机对话 需要减少万能回复 通过实验发现首词对生成结果的影响， 这里联想到《Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the Generalizability of Relation Extraction》中也用提到了反事实的训练。 报告体会 虽然很多领域我都没有接触过，也不是很能够听得懂，但是还是学到了不少研究的方法，也感觉到了基础的重要性。虽然如今深度学习当道，但是一些传统的方法仍然能够给我们带来很多的启发，基于这些传统的方法去搭建网络模型往往可以得到不错的结果。","link":"/2021/03/14/NLP&KG%E5%89%8D%E6%B2%BF%E8%AE%BA%E5%9D%9B%E8%AE%B2%E5%BA%A7%E7%AC%94%E8%AE%B0/"},{"title":"Python知识点","text":"是上学期Python期末复习记录的一些知识点，可以快速回顾正则表达式、Pandas、Numpy等的用法。 tuple是[]，list是()，tuple在只有一个元素时要加逗号消除歧义，比如 12a=(1,)#a is a tuplea=(1)#a is a int Python里面多为左闭右开 zip和*zip 字典操作 get的用法 字典的遍历 1234for i in dict_1: print(i)#print keyfor i in dict_1.items(): print(i)#print (key,value) 字典元素的增加和修改 is 和 == 的区别 is判断地址是否一样，==判断数值是否相等，不同解释器会导致结果不一样。 （应该不是考点，不过是个有趣的小细节） 开头要写 1#coding=utf-8 字符串 查找find()与index()，区别前者未找到返回-1，后者未找到返回错误，找到都返回下标。 split()用指定字符分割，默认为任何空白符号（空格、制表、换行），分为两部分（无分割字符），partition()则查找第一个对应的符号，然后分为前中后三部分。 不过split返回list，partition返回tuple。 split(\"char\", num)指定最大分割次数。 其他操作 str.strip(c)，其中c也是一个字符或者一个字符串。它删除的是str字符串从头开始或者从尾开始带有c含有的字符的所有字符。 字符串的格式化 循环结构 123456if 'e1': 'statement 1'elif 'e2': 'statement 2'else: 'statement 3' and 和 or 惰性求值 推导式快速创建 序列解包 函数 123def fun(x): '''注释 ''' 和C++一样，一般情况都是传递形参，但是诸如数组等传递的是指针，如下图 对于默认形参的要求也类似与C++ 一些值得注意的小细节 在Python中，可以指定形参进行传递，此时顺序无所谓。 变量作用域，容易迷惑人。 正则表达式 类 命令行模式下， _ 表示解释器最后一次显示的内容或最后一次语句正确执行的输出结果，在程序中，可以用 _ 表示不关心该变量的值。 Numpy索引 维数小于等于3，arr[]第一个选行，然后第二个选列。 花式索引（Fancy indexing）是指利用整数数组进行索引，其他切片与索引都是浅复制。使用ndarray.view()创建新的数组，不再是指针简单的复制。 Numpy与Pandas Pandas中DataFrame的数据访问","link":"/2021/01/16/Python%E7%AC%94%E8%AE%B0/"},{"title":"对偶问题及SVM","text":"本文主要介绍SVM以及涉及到的一些凸优化知识. Lagrange对偶问题 考虑标准形式的优化问题, 这里称为原问题, 其最优值为. 其Lagrange函数为 其Lagrange对偶函数为 这一定是一个凹函数. 更详细的理论推到参考Convex Optimization. 如果, 那么有 也就是说Lagrange对偶函数给出了原问题的下界. 那么自然而然我们就想要知道这个下界最大是多少, 于是我们就有了Lagrange对偶问题 对偶问题的最优值我们用来表示. 我们天然有 这就是弱对偶性. 如果 那么强对偶性成立. 这就是极好的一条性质, 因为我们可以把求问题的最优解转换为求对偶问题的最优解. 那么我们需要关注的就是什么时候强对偶性成立. 如果原问题是凸问题, 并且符合一些条件那么强对偶性就成立. 典型的条件有Slater条件等. 但是如果原问题是个非凸问题, 那么我们有一个强对偶性成立的必要条件: KKT条件. 即原问题和对偶问题对偶间隙为0的一对最优解和满足 对于凸问题, KKT条件是充要条件, 也就是可以通过求解KKT条件来获得凸问题的最优解. SVM SVM的目的很符合直觉, 就是想要分得更开, 即两类数据正确划分的的同时使两类数据点间隔最大. 如图可以看出, 三个样本构成了support vector, 来决定了划分的间隔. Hard SVM 求解 假设所有的样本都能被正确分类, 这就是Hard SVM. 那么所有的样本都满足 作为support vector的样本取到等号. 间隔也容易计算出为. 我们要使两类数据正确划分的的同时使两类数据点间隔最大 ,即 同时等价于 这就是Hard SVM的形式. 这虽然是个凸二次规划, 但是计算复杂度较高, 于是我们将其转换为对偶问题来进行求解(虽然都是QP问题, 但是简化约束后可以利用SMO等高效算法). 首先先得到Lagrange函数 接着要求其对偶问题. 由于对偶问题定义, 我们要 令 对 和 的偏导为零, 得到 然后代入, 就可以得到对偶问题 然后解出即可求出 和 $ b b$根据support vector求). 结果分析 根据前面的介绍, 我们知道肯定满足KKT条件, 即 根据KKT中的互补松弛条件, 即, 发现只有满足的样本(即support vector)才能使得, 因为, 也就是说, 只有support vector对于最终模型起到了作用. 测试新样本 对于一个要进行分类的新样本, Missing open brace for subscript \\begin{aligned} \\hat{y_{t}}=\\operatorname{sign}\\left(\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}_{t}+b\\right)&amp;=\\operatorname{sign}\\left(\\sum_{i=1}^m \\alpha_{i} y_i \\boldsymbol{x}_^\\mathrm{T} \\boldsymbol{x}_{t}+b\\right)\\\\ &amp;=\\operatorname{sign}\\left(\\sum_{i\\in SupportVectors} \\alpha_{i} y_i \\boldsymbol{x}_^\\mathrm{T} \\boldsymbol{x}_{t}+b\\right) \\end{aligned} ### Soft SVM 通常情况, 并不存在一个能够完美划分样本的超平面, 这种情况就是Soft SVM, 即允许一定的错误. 很直觉的一个想法是最小化间隔和错误分类的个数, 即 但是这样很难变为一个QP问题求解, 于是就用别的函数代替, 如hinge损失, 指数损失, 对率损失等. 若采用hinge损失, 则变成 引入“松弛变量”(slack variables) , 可重写为 这就是Soft SVM的形式. 引入松弛变量后式子的物理含义可以通过下图理解 这里的和正则化中的一样, 是一个由自己选择的超参数. 与Hard SVM类似, 我们首先先得到Lagrange函数 其中 , 是拉格朗日乘子. 令 对 的偏导为零可得 代入求得对偶问题 然后可以利用成熟的算法求解这个约束简单的QP问题. 结果分析 与Hard SVM的对偶问题对比可看出, 两者唯一的差别就在于对偶变量的约束不同: Soft SVM是 $ 0 {i} C 是 0 {i} . $ 并且, 这里同样会满足KKT条件, 于是, 对任意训练样本 $ ({i}, y{i}) 总有 {i}=0 或 y{i} f({i})=1-{i} . $ 若 , 则该样本不会对 有任何影响; 若 $ {i}&gt;0 则必有 y{i} f(x_{i})=1-{i} 即该样本是支持向量并且若 {i}&lt;C 则 {i}&gt;0 进而有 {i}=0 即该样本恰在最大间隔边界上若 {i}=C 则有 {i}=0 此时若 {i} 则该样本落在最大间隔内部若 {i}&gt;1 $ 则该样本被错误分类. 同样, 只有support vector对于最终模型起到了作用. Kernel Trick 并不是所有的样本都是线性可分的, 核方法是希望将样本从原始空间映射到一个更高维的特征空间, 使得样本在这个特征空间内线性可分. 并且, 特征维数有限的情况下, 一定存在高维空间是样本可分. 令 表示将 映射后的特征向量, 于是, 在特征空间中划分超平面所对应的模型可表示为 于是, 对偶问题变为 我们并不需要知道是如何映射的, 只要知道映射后向量的内积即可, 于是我们定义函数来表示映射后向量的内积 最后求得的模型即为 核函数本质上定义了一个再生核希尔伯特空间, 常用的核函数有线性核, 高斯核等.","link":"/2021/05/22/SVM/"},{"title":"Transformer库初探(2)","text":"Transformers上提供了数千种经过预训练的模型，提供了pipeline的API方便快速实验，上周初步接触，最近主要依据官方手册进一步学习，了解熟悉整个流程。 本次笔记主要参考https://github.com/huggingface/transformers/tree/master/examples，对GLUE Benchmark进行fine-tune。 数据集 GLUE分为九个任务，每个任务采取不同的指标 for CoLA: Matthews Correlation Coefficient for MNLI (matched or mismatched): Accuracy for MRPC: Accuracy and F1 score for QNLI: Accuracy for QQP: Accuracy and F1 score for RTE: Accuracy for SST-2: Accuracy for STS-B: Pearson Correlation Coefficient and Spearman's_Rank_Correlation_Coefficient for WNLI: Accuracy 下图是CoLA训练数据的一部分。 预处理 由Transformers提供的Tokenizer完成，用AutoTokenizer.from_pretrained快速实例化。 12from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True) 测试一下分词的结果 Fine-tune 任务都是句子分类的，所以使用了AutoModelForSequenceClassification类，用from_pretrained方法下载模型，需要注意的的是MNLI是个三分类问题，STS-B是用分类方法做的回归问题，其他都是二分类问题。 123from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainernum_labels = 3 if task.startswith(&quot;mnli&quot;) else 1 if task==&quot;stsb&quot; else 2model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels) 接下来设定一些训练参数，评价指标。 1234567891011121314151617181920metric_name = &quot;pearson&quot; if task == &quot;stsb&quot; else &quot;matthews_correlation&quot; if task == &quot;cola&quot; else &quot;accuracy&quot;args = TrainingArguments( &quot;test-glue&quot;, evaluation_strategy = &quot;epoch&quot;, learning_rate=2e-5, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=5, weight_decay=0.01, load_best_model_at_end=True, metric_for_best_model=metric_name,)def compute_metrics(eval_pred): predictions, labels = eval_pred if task != &quot;stsb&quot;: predictions = np.argmax(predictions, axis=1) else: predictions = predictions[:, 0] return metric.compute(predictions=predictions, references=labels) 连同数据集传递给Trainer 123456789validation_key = &quot;validation_mismatched&quot; if task == &quot;mnli-mm&quot; else &quot;validation_matched&quot; if task == &quot;mnli&quot; else &quot;validation&quot;trainer = Trainer( model, args, train_dataset=encoded_dataset[&quot;train&quot;], eval_dataset=encoded_dataset[validation_key], tokenizer=tokenizer, compute_metrics=compute_metrics) 就可以开始训练了。 Hyperparameter search 与上述过程差别不大，调用hyperparameter_search即可自动搜索。 1best_run = trainer.hyperparameter_search(n_trials=10, direction=&quot;maximize&quot;) 小结 Transformers提供了一个标准化的流程，可以很快的上手各种模型，官方文档很全。但是对于数据集预处理方面感觉自己能力有所欠缺，还是需要多读代码，并要有能够套用代码的能力。","link":"/2021/02/21/Transformer%E5%BA%93%E5%88%9D%E6%8E%A2(2)/"},{"title":"Transformer库初探(1)","text":"本文主要参考https://github.com/bentrevett/pytorch-sentiment-analysis中的6 - Transformers for Sentiment Analysis，使用colab进行实验。 实验内容 使用BERT训练IMDB评论的情感分析。 实验步骤 数据预处理 进行分词及输入长度限制，可以看到此处输入长度最长为512。（这里就出现了BERT不能处理长文本的问题，CogLTX: Applying BERT to Long Texts这篇论文提出了一种解决办法，后续会尝试采用这种方法再次实验） 进行一些token的处理以及标签构建。 实验数据从torchtext中获取。 123from torchtext import datasetstrain_data, test_data = datasets.IMDB.splits(TEXT, LABEL)train_data, valid_data = train_data.split(random_state = random.seed(SEED)) 此实验中batch_size为128。 模型建立 载入pre-trained model。 12from transformers import BertTokenizer, BertModelbert = BertModel.from_pretrained('bert-base-uncased') 进行模型的定义以及实例化。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class BERTGRUSentiment(nn.Module): def __init__(self,bert:BertModel, hidden_dim:int, output_dim:int, n_layers:int, bidirectional:bool, dropout:float): super(BERTGRUSentiment, self).__init__() self.bert=bert embedding_dim=bert.config.to_dict()['hidden_size'] self.rnn=nn.GRU(embedding_dim,hidden_dim,n_layers, bidirectional=bidirectional, batch_first=True, dropout=0 if n_layers&lt;2 else dropout) self.fc=nn.Linear(hidden_dim*2 if bidirectional else hidden_dim,output_dim) self.dropout=nn.Dropout(dropout) def forward(self,text): with torch.no_grad(): embedding=self.bert(text)[0] #embeddiing:(batch,seq,embedding_dim) _,hidden=self.rnn(embedding) #(bi*num_layers,batch,hidden_size) hidden=self.dropout(torch.cat((hidden[-1,:,:],hidden[-2,:,:]),dim=1) if self.rnn.bidirectional else hidden[-1,:,:]) return self.fc(hidden) #（batch,output_dim）HIDDEN_DIM = 256OUTPUT_DIM = 1N_LAYERS = 2BIDIRECTIONAL = TrueDROPOUT = 0.25model = BERTGRUSentiment(bert, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT) 在训练之前需要将模型中有关于BERT预训练的参数冻结。 此次实验共有2,759,169参数需要训练。 模型训练 训练花费了较长时间，尝试使用CPU进行，无法完成。 训练过程占用了大量显存。 模型测试 第三个是我在IMDB上找的影评，可以看到对于较长的句子仍然可以正确划分其情感。 123456predict_sentiment(model, tokenizer, &quot;This film is terrible&quot;)# 0.02264496125280857predict_sentiment(model, tokenizer, &quot;This film is great&quot;)# 0.9411056041717529predict_sentiment(model, tokenizer, &quot;People are grossly overrating this movie. It's pretty boring for the first hour. Not even close to as entertaining as other Pixar movies. I don't think I laughed out loud one time during the whole movie. It's watchable, but I wouldn't rewatch it ever again.&quot;)# 0.0345646762135478 实验中遇到的问题 计算loss时出现的错误，预测的batch维度与真实的batch维度不同，可能由于与参考代码部分环境版本不一致导致。 在程序中加入predict = predict.squeeze(-1) 即可解决问题。 实验收获及小结 上周学习了Transformer以及BERT的一些知识，本周使用Transformer库中的BERT模型进行了情感分类的实验，可以感觉到BERT的强大。同时，也能发现BERT模型相当巨大，尽管冻结了BERT模型中的参数，仍然需要训练不少的参数，GPU在这个过程中的优势彻底展现了出来。 当然，此次使用的BERT模型也存在不能处理长文本的问题，以本次实验为例，IMDB上很多影评的长度是超过512的，那么就需要对BERT进行一些改进，本周尚未能实现CogLTX: Applying BERT to Long Texts中的方法，下周继续尝试。","link":"/2021/02/28/Transformer%E5%BA%93%E5%88%9D%E6%8E%A2/"},{"title":"从Attention到Transformer再到BERT","text":"Attention 《Recurrent Models of Visual Attention》这篇论文让attention机制真正火起来，该论文主要在RNN上使用attention机制进行分类。attention机制的直观理解：人在观察图像时会将注意力集中在重要的部分，attention使得机器也能够更加关注重要的部分。有点类似于自动移动CNN的Filter寻找关键特征。以下是该论文的attention模型结构，可以对比传统RNN理解。 后来attention也被广泛运用于NLP中，不过在很长一段时间，attention都与CNN/RNN共同被使用，直到《Attention Is All You Need》提出了完全基于attention机制的transformer模型。 Transformer 模型结构如下， Q：Positional Encoding的作用是什么？ A：Positional Encoding这部分赋予每个位置不同的值，让模型可以拥有句子顺序信息。因为attention是并行的，所以如果不加入则会丢失顺序信息。 Q：Multi-head attention与attention的区别？ A：Multi-head attention相当于做了多个attention，但是每个attention关注的方面不同。 在transformer中，attention具体的实现如下所示， 缩放因子使得点乘结果的值变小，否则经过softmax后的梯度很小，难以反向传播。更直观的self-attention结构如下，对于输入文本，分别将每个字作为Query，加权融合文本中所有字的语义信息，得到各个字的增强语义向量。 BERT Model Architecture BERT是叠加多个transformer的encoder部分组成的模型，结构如下， Training 对于一个如此巨大的模型，如何train是关键问题。BERT的作者提出了两种方式来训练语言模型。 Apparent1: Masked LM 通俗地说就是在输入一句话的时候，随机地选一些要预测的词，然后用一个MASK来代替它们，让模型根据所给的标签去学习这些地方该填的词了。 由于Linear Classifier分类能力较弱，所以如果能正确分类，说明BERT抽取出来的representation很好，可以轻易地知道被MASK掉的是哪个词汇。如果两个词填在被抽去的地方意思相近，它们具有相似的embedding。 Apparent2: Next Sentence Prediction 判断两个句子是不是连在一起的。 Reference [1] Mnih V, Heess N, Graves A, et al. Recurrent models of visual attention[J]. arXiv preprint arXiv:1406.6247, 2014. [2] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. arXiv preprint arXiv:1706.03762, 2017. [3] Devlin J, Chang M W, Lee K, et al. Bert: Pre-training of deep bidirectional transformers for language understanding[J]. arXiv preprint arXiv:1810.04805, 2018. [4] Hung-yi Lee.Machine Learning (2020,Spring)[EB/OL].http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html,2020. [5] 腾讯Bugly.图解BERT模型：从零开始构建BERT[EB/OL].https://cloud.tencent.com/developer/article/1389555,2019-01-30.","link":"/2021/02/07/%E4%BB%8EAttention%E5%88%B0Transformer%E5%86%8D%E5%88%B0BERT/"},{"title":"方差与偏差","text":"简介 • 偏差：描述了拟合模型与真实模型接近的程度。通俗地讲，偏差越低，拟合得到的参数就越接近真实参数，同时预测值也就越靠近真实值。 • 方差：描述了在不同的训练集下，拟合模型预测值的离散程度。一般而言，改变训练集会影响到拟合出的参数，方差越高，则改变训练集所造成的参数变化就越剧烈。 偏差-方差示意图如下, 中心红点可看作是模型真实的参数，蓝点则是在某个数据集上训练出的参数. 偏差(bias)的定义 方差(variance)的定义 其中(这里的表示参数是, 这也很容易理解, 训练出来的模型依赖于训练集). 表示根据某一训练集训练出来的模型. 期望误差分解 假设, 是噪音. 注意, 这里是对求期望(期望的本质是求一个积分, 所以得到的结果是一个与无关的量), 所以 此时, 是一个与无关的量, 即可以看作一个常量. 根据定义,则显然有 结合上面几个式子, 我们就可以得到 也就是说, 误差可以分解为偏差, 方差与噪音之和.","link":"/2021/05/16/%E5%81%8F%E5%B7%AE%E4%B8%8E%E6%96%B9%E5%B7%AE/"},{"title":"矩阵求导","text":"矩阵求导常用公式 向量一般用小写粗体表示, 矩阵用大写粗体表示. 参考: 矩阵求导公式的数学推导（矩阵求导——基础篇）https://zhuanlan.zhihu.com/p/273729929","link":"/2021/05/16/%E7%9F%A9%E9%98%B5%E5%9F%BA%E7%A1%80/"},{"title":"线性回归","text":"线性回归是机器学习中最基本的一类的问题. 这里用到公式 并且是对称阵. 令, 则有 这个方法即Normal Equation(正规方程). 从概率论角度解释线性回归","link":"/2021/05/16/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"},{"title":"《Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the Generalizability of Relation Extraction》 阅读笔记","text":"前两周在根据transformers库的文档熟悉操作，但是有种不知其所以然的感觉，本周慢下来读论文，熟悉常用的研究方法及思路，加深对pre-trained model的理解，也借助论文，了解领域的相关名词及概念。 一些概念 Wiki80数据集 Wiki80是由FewRel而来，而FewRel 是以 Wikipedia 作为语料库，以 Wikidata 作为知识图谱构建的，是一个大规模精标注关系抽取数据集 。 任务：对于给定了的句子和两个做了标注的名词，从给定的关系清单中选出最合适的关系。 数据集中一共包含80中关系，经统计各个关系个数均为700，合计56000个样本。 Wiki80数据集采用人工精标，不包含噪声。 样本格式： 例子：{“token”: [“Vahitahi”, “has”, “a”, “territorial”, “airport”, “.”], “h”: {“name”: “territorial airport”, “id”: “Q16897548”, “pos”: [3, 5]}, “t”: {“name”: “vahitahi”, “id”: “Q1811472”, “pos”: [0, 1]}, “relation”: “place served by transport hub”} TACRED数据集 TACRED是一个大规模的关系提取数据集，具有106264个示例，这些示例是通过新闻电讯和Web文本构建的，这些示例来自用于每年TAC知识库人口（TAC KBP）挑战的语料。TACRED中的示例涵盖了TAC KBP中使用的41个关系类型（例如，per：schools_attended和org：members），如果没有定义的关系则将其标记为no_relation。这些示例是通过将TACKBP和群众外包获得的。 在每个TACRED示例中，提供了以下注释： 主题和客体提及的范围； 提及的类型（在 Stanford NER system中使用的23种细粒度类型中）； 实体之间保持的关系（在41个TAC KBP规范关系类型中），或者如果未找到任何关系，则为no_relation标签。 显著图 显著图（Saliency Map）是一种可解释方法，最初是计算机视觉领域用于解释模型预测效果的一个工具，它的大小和原始图相同，图上的点的数值对应原图像素对预测目标类别的重要性。 最初的显著图是通过有监督方式计算出的模型损失对输入图像的梯度，梯度数值经过归一化处理，用于衡量每个输入单位的变化对目标输出类别概率的影响，其值越大越能反映该输入的相对重要性，即 下图为一个MNIST手写数字识别的例子： 这也可以用于非目标类别，比如将数字7预测为1，那么7的一横在显著图上就应该是负的，一竖则是正的。 积分梯度 显著图中，直接通过梯度来反映重要性，但是这在一些情况下是不适用的。比如下图，鼻子的长度对于判断是否是大象很重要，但是鼻子长度达到一定程度后，长一点短一点对于判断的影响不是很大了，那么此时梯度就接近于0。 所以，采用整条梯度线的积分作为鼻子对大象分类的重要程度，将会是更合理的一种获取显著图的方式。 这里整个偏导被换成了变分的形式，变分边界是基线图像（可以选择不同的基线）和当前图像，变分路径可以任意选择（一般就选择线性路径插值，将原始图片x乘以0-1区间的倍率）。 NLP也可以用积分梯度及显著图进行可解释性研究。 《Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the Generalizability of Relation Extraction》 摘要及介绍 本文是一篇关系抽取泛化性能分析的文章，作者举了一个例子来说明当前该领域研究的不足，“我们不知道模型出色的性能与其泛化能力是否预示着良好的泛化性能”，在文章中，作者提出了五个问题， Q1：模型是否在通过一些关键词进行模版匹配？ Q2：模型在关系抽取问题上的对抗样本表现如何？ Q3：模型对反事实样本的表现如何？ Q4：模型是否在捕捉和利用数据中的一些不存在的关联来进行预测？ Q5：模型是否存在语义的偏差，即模型只从实体文本或者上下文本中学习关系（例如，光靠记实体名字就能推断）？ 主要进行的工作如下，主要从鲁棒性（robustness）及偏差（bias）两方面进行研究。 截屏2021-02-28 上午10.36.58 正文 鲁棒性 分析的主要方法如下， Q1 这一部分基于CheckList工具设计了对上下文和头尾实体中的文本进行同类型随机替换的实验。作者通过随机查看生产的结果，基本标签都仍然是正确的。通过替换产生的随机样本数量大概在原样本的5～8倍，作为一种数据增强的方法，使用BERT进行微调训练和测试。 在Wiki80和TACRED数据集上进行实验的结果如下： 这个可以看出数据增强训练的模型在原始数据集上分数略有下降，但鲁棒性更好，这个结果很符合直觉。 Q2 这一部分，基于OpenAttack工具包设计了适应关系抽取任务的文本对抗实验，将文本多分类拓展到关系抽取领域，并支持原有的文本对抗模型（TextFooler，PWWS，HotFlip等等）。 对于对抗模型了解匮乏，部分细节在系统地学习对抗模型后补充。不过，这不影响我们从实验结果中学习分析的方法。 可以看到原始模型在对抗样本上表现较差，效果下降了许多；而经过对抗的训练后，在对抗样本上的效果较好，在原始数据上甚至还有更好的表现。但是可以发现对抗训练的模型在TACRED的原始数据集上表现变差了，这在Wen, Y.; Li, S.; and Jia, K. 2019. Towards Understand- ing the Regularization of Adversarial Robustness on Neural Networks .也有提及，作者认为这之间是一个平衡，还需要更多的研究。 Q3 对于关系抽取，这里的反事实样本指的是改变句子的内容，使得句子中实体的关系从【关系A】变为【不存在A关系】（或者【没有关系】即no_relation）。 本文通过如下方法构造反事实样本： 使用训练好的BERT模型，对句子计算了每一个token的梯度以后，去掉停用词和BERT模型的保留token（[SEP]，[CLS]，[unused0/1/2/3]等），对剩余结果按照梯度得分进行排序获得关键的token； 将关键token进行遮盖（使用特殊标记进行替换和直接去除效果差异不大，实际使用去除token并补齐句长的方法），得到反事实的样本，其关系标签为“非关系”（即no_relation）； 使用BERT模型，对反事实样本进行测试，并对训练数据进行上述方式的反事实增广后，使用模型进行微调训练和测试。 需要注意的是，由于在Wiki80数据中不存在非关系，所以本部分仅针对TACRED数据集进行实验。 这个例子说明为什么用积分梯度来获得反事实的样本，使用attention就达不到所需要的效果。 初始模型在反事实集合上的表现不佳，而经过训练后的模型在反事实样本上表现提高较大，且在原始数据上的表现也与原始模型差距不大。 偏差 分析的主要方法如下， Q4 这一部分基于数据中存在的真实情况提出了假设，即模型会利用一些无关的词语共现情况进行辅助推理，这些词语实际上对句子表达的关系并没有实际影响。 TACRED数据集是通过远程监督清洗得到的，在样本的抽取过程中存在一些无关的共现（即部分词语和某种关系总是同时出现），这可能使得模型可能在训练中学习到数据偏差带来的本不存在的关联。 作者经过统计，发现确实存在这样的情况，所以经过人工的调整（即筛选出上述关键词中和关系无关的关键词）以后，对测试数据中按不同关系的样本，将其包含对应高频无关词进行遮盖（这里实验中用空符号对原始token进行替换，不改变原始token的位置），并使用原始模型进行测试；同时，对训练数据集按照测试集同样遮盖处理，获得的微调模型分别对原始测试数据和遮盖高频词后的测试数据进行测试，结果如下： 可以看出，经过去除高频非相关词后，原始模型的测试结果有所下降；经过同样处理的训练数据集微调训练得出的模型在基本保持对原始数据分布的效果的基础上，在新的测试集上同样具有较高的效果。 作者自己在博客中写到”实验后感觉，对高频词进行mask可能会破坏语句的结构，所以效果下降是否就是由于数据偏见引起的，这部分有待商榷，当然也期待会有更好的方案（如果使用词替换的方案也许更合理，但是对人工标注的要求较高）”。他自己也提到” 由于数据处理方法在训练集和测试集上统一，形成了相似的空间分布，所以微调后训练结果提升也许不是多么神奇的事情……“，如此来看，这个结果确实不是那么具有说服力，当然这个结果是符合直觉的，但是实验方法不是那么严谨。 Q5 这一部分尚未完全理解，一些说法参考了作者本人博客。 作者参考了《More Data, More Relations, More Context and More Openness- A Review and Outlook for Relation Extraction》中模型对实体文本、上下文的实验，并提出了基于频率的实体遮盖（Frequency-based Masking）方法。 本文的实验同样设置了mask entity和only entity的场景，但是存在一些不同： mask entity的具体实现为将头尾实体进行去除，但是保留句子的头尾位置信息（即，BERT中的特殊token）； mask entity设置了随机去除一半实体（50%），以及全部去除（100%）的场景； mask entity设置了基于实体对频率（frequency）的场景，即出现频率越高的实体对所在的样本的遮盖率越高，以期消除模型对实体文本的“过度学习”。 这里由于Wiki80中实体对在数据中高频率出现的现象并不明显，所以不对这一数据集进行实验，而在TACRED数据集由于其构造方式（远程监督），存在较多实体对高频出现的情形。 设置De-biased组别，这一组别旨在检测模型在提供实体名字时预测失败的情形下，提供了上下文文本以后是否能够成功预测（即，筛选出only entity场景下分类错误样本的原始样本进行测试）。 可以看出，（1）初始模型对mask entity场景的鲁棒性不足，而提供了实体名字的组别（only entity）中表现更好，这证明了模型存在“记名字”的现象；（2）经过mask增广数据训练的模型在mask entity表现提升，但依然不及原始效果，在only entity场景提升不明显或者没有提升；（3）所有mask entity增广训练的模型都在De-biased测试上表现增强，其中按频率mask的组别在De-biased测试上表现更好（当然效果区别不是很明显）。 小结 第一次读这种类型的论文，虽然没有创造性的提出多么新颖的模型，但是作者思考问题，分析问题的思路值得学习，并且这些方法是可以能够模仿的，很多结果也是十分符合直觉的。这也是第一次读关系抽取的论文，也对于关系抽取有了一定的了解。有时间希望可以复现一下作者的代码，加深理解。","link":"/2021/03/05/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"关于分类-机器学习","text":"该分类下的博文偏数学推导以及一些较难的知识点, 对于基础知识的介绍十分简略, 请见谅.","link":"/2021/05/16/%E8%AF%B4%E6%98%8E/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Machine Leaning","slug":"Machine-Leaning","link":"/tags/Machine-Leaning/"},{"name":"Mathematics","slug":"Mathematics","link":"/tags/Mathematics/"}],"categories":[{"name":"李宏毅机器学习课程","slug":"李宏毅机器学习课程","link":"/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/"},{"name":"自然语言处理","slug":"自然语言处理","link":"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"},{"name":"其他","slug":"其他","link":"/categories/%E5%85%B6%E4%BB%96/"},{"name":"Coding","slug":"Coding","link":"/categories/Coding/"},{"name":"机器学习","slug":"机器学习","link":"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]}