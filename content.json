{"pages":[],"posts":[{"title":"","text":"Recipe of Deep Learning三种防止过拟合的办法：Early stopping, Regularization, Dropout Early stopping在Validation set上的Loss最小时中止 RegularizationL1 norm和L2 norm均可用于Regularization中，其中L2 norm较为常用，其如下式，对比L1和L2的update过程：其中Sgn(阶跃函数)为绝对值的导数。 L1和L2，虽然它们同样是让参数的绝对值变小，但它们做的事情其实略有不同： L1使参数绝对值变小的方式是每次update减掉一个固定的值 L2使参数绝对值变小的方式是每次update乘上一个小于1的固定值 因此，当参数的绝对值比较大的时候，L2会让下降得更快，而L1每次update只让w减去一个固定的值，train完以后可能还会有很多比较大的参数；当参数的绝对值比较小的时候，L2的下降速度就会变得很慢，train出来的参数平均都是比较小的，而L1每次下降一个固定的value，train出来的参数是比较sparse的，这些参数有很多是接近0的值，也会有很大的值。 Q：为什么要让参数w绝对值变小？ A：正则项有选择地让某些 变小，样本中的特征有很多，但大部分特征都是无关紧要的，只有一小部分关键的特征支撑起了整个预测模型。正则化让无关紧要的变小，而作为关键特征的如果变小，则可能导致损失函数急剧增大，么由此造成的损失函数的扩大将远大于从正则项上获得的微小收益，所以这些关键的可以几乎不受正则项的干涉。 DropoutDropout的思想和集成学习十分类似，在training的时候，每次update参数之前，我们对每一个neuron(除了最后的output layer)做sampling(抽样) ，每个neuron都有p%的几率会被丢掉，如果某个neuron被丢掉的话，跟它相连的weight也都要被丢掉，实际上就是每次update参数之前都通过抽样只保留network中的一部分neuron来做训练，最后将多次训练的不同参数乘以系数(1-p%)构成完整的网络。 如果network是非linear的，ensemble和dropout是不equivalent的；但是，dropout最后一个很神奇的地方是，虽然在non-linear的情况下，它是跟ensemble不相等的，但最后的结果还是会work如果network很接近linear的话，dropout所得到的performance会比较好，而ReLU和Maxout的network相对来说是比较接近于linear的，所以我们通常会把含有ReLU或Maxout的network与Dropout配合起来使用。","link":"/2021/05/15/2020.1.23(2)/"},{"title":"The foundation of Convolutional Neural network","text":"CNN用在图像处理或类似方式表达的问题当中(比如棋盘)，一般性流程如下， ConvolutionConvolution的本质是在一个图像中不断移动一个小框，然后看这个小框内是否存在一些特征。 自动学习出多个Filter，然后每个Filter原来图像的matrix中移动，计算内积得到新的matrix，最后得到一个多维的结果，过程如下， 注：如果原图像有多个维度，则每个Filter也有同样的维度。 Filter的过程通过神经网络实现， MAX PoolingMAX Pooling的过程类似于抽调图片的一些像素，图像整体变化不大，人类仍然可以识别，但是这个操作却减小了计算量。 MAX Pooling的过程是压缩Convolution得到的结果，用最大的值代替一个区域的结果。整个CNN的过程如下， What does CNN learn？深度学习并非完全不可理解，但是它学习到的东西和人理解的确实有所差异，比如手写数字识别问题，用训练好的网络反推最佳匹配的输入应该是什么，发现和想象的相差巨大， 运用之妙，存乎一心比如围棋，围棋如果进行MAX Pooling操作则会丢失掉许多棋子的信息，那么我们在CNN训练下围棋时就应该删去MAX Pooling操作，而事实上Alpha Go也是这么做的。 再比如这样的语音处理，Time上会有后续操作进行处理，而在Frequency上男女生发音音调不同，但是不影响内容一致，故在Frequency上使用Filter寻找特征。 词向量模型使用CNN也是同理，不需要像红色框一样进行纵向的查找。 针对不同的application要设计符合它特性的network structure，而不是生硬套用，这就是CNN架构的设计理念： 应用之道，存乎一心","link":"/2021/01/23/2021.1.23(1)/"},{"title":"The foudation of Recurrent Neural Network","text":"Recurrent Neural Network是一种具有记忆能力的神经网络，主要用途在NLP上，句子中的单词意思依赖于前后句子，所以需要使用RNN这种带有记忆能力的神经网络模型。 Bidirectional RNN是双向的RNN，普通的RNN句子中单词的意义只依赖于之前的单词，而如果使用Bidirectional RNN神经网络则具有看整个句子的能力。 Long short-term Memory(LSTM)目前来说，一般提到的RNN都指LSTM，其结构如下， LSTM通过三个gate来控制输入输出以及是否遗忘，可以与RNN的模型比较着看。 Training RNNRNN的训练过程中Loss会剧烈抖动，画出它的图像可以看出会存在“悬崖”与“平原”，如图所示， 这是一个简单的例子解释为什么RNN会有这样的特性 故RNN经常出现梯度消失和梯度爆炸。 LSTM可以拿掉error surface上比较平坦的地方，从而解决梯度消失的问题。 Q：为什么要把RNN换成LSTM？ A：LSTM可以解决梯度消失的问题 Q：为什么LSTM能够解决梯度消失的问题？ A：RNN和LSTM对memory的处理其实是不一样的： 在RNN中，每个新的时间点，memory里的旧值都会被新值所覆盖 在LSTM中，每个新的时间点，memory里的值会乘上$f(g_f)$与新值相加 对RNN来说，w对memory的影响每次都会被清除，而对LSTM来说，除非forget gate被打开，否则$w$对memory的影响就不会被清除，而是一直累加保留，因此它不会有梯度消失的问题。","link":"/2021/01/25/2020.1.25/"},{"title":"Semi-supervised","text":"这里运用到了两个假设 Low-density Separation Assumption假设世界非黑即白，那么我们的分界应该很清晰。 如下图，左边的分界就比右边的好。 Smoothness Assumption近朱者赤近墨者黑 如下图，尽管$x_3$可能里$x_2$更近，但是$x_1$有连续的点过渡到$x_2$，所以$x_2$更可能和$x_1$是一类。","link":"/2021/02/01/2021.2.1/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"}],"categories":[{"name":"李宏毅机器学习课程","slug":"李宏毅机器学习课程","link":"/categories/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B/"}]}